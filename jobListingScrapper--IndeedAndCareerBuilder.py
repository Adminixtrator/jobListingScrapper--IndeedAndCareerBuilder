# -*- coding: utf-8 -*-
"""jobListingScrapper--IndeedAndCareerBuilder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zOckqxi-jaiP98G2AMjknE9DA7QdU69A

# Not exactly ***Open Source*** yunno, so no comments âœŒï¸ðŸ˜ŒâœŒï¸
"""

import requests
from bs4 import BeautifulSoup
import time
import urllib


class IndeedScraper():

  def extract_job_title_from_result(soup):
    jobs = []
    for div in soup.find_all(name="div", attrs={"class": "row"}):
      for a in div.find_all(name="a", attrs={"data-tn-element": "jobTitle"}):
        jobs.append(a["title"])
    return jobs

  def extract_company_from_result(soup): 
    companies = []
    for div in soup.find_all(name="div", attrs={"class": "row"}):
      company = div.find_all(name="span", attrs={"class": "company"})
      if len(company) > 0:
        for b in company:
          companies.append(b.text.strip())
      else:
        sec_try = div.find_all(name="span", attrs={"class": "result-link-source"})
        for span in sec_try:
          companies.append(span.text.strip())
    return companies

  def extract_refUrl_from_result(soup):
    urls = []
    for div in soup.find_all(name="div", attrs={"class": "row"}):
      for a in div.find_all(name="a", attrs={"data-tn-element": "jobTitle"}):
        urls.append(a['href'])
    return urls

  def extract_locations_from_result(soup):
    locations = []
    for div in soup.find_all(name="div", attrs={"class": "sjcl"}):
      for a in div.find_all(name="div", attrs={"class": "recJobLoc"}):
        locations.append(a['data-rc-loc'].strip())
    return locations

  def extract_jobPolicy_from_result(soup):
    jobPolicy = []
    for div in soup.find_all(name="div", attrs={"class": "sjcl"}):
      for a in div.find_all(name="span", attrs={"class": "remote"}), div.find_all(name="div", attrs={"class": "recJobLoc"}):
        if a == []:
          jobPolicy.append(' ')
        else:
          try:
            junk = a[0]['data-rc-loc']
          except:
            jobPolicy.append(a[0].text.strip())
    return jobPolicy

# UNABLE TO SCRAPE

# def extract_salary_from_result(soup): 
#   salaries = []
#   for div in soup.find_all(name="div", attrs={"class": "row"}):
#     for a in div.find_all(name="a", attrs={"data-tn-element": "jobTitle"}):
      # _URL = "https://www.indeed.com"+a['href']
      # print(_URL)
      # _page = requests.get(_URL)
      # _soup = BeautifulSoup(_page.text, "html.parser")
      # for b in _soup.find_all(name="div", attrs={"class": "jobDetailsSection"}):
        # salary = b.find_all(name="span")
        # print(b.text.strip())
        # salaries.append(salary.text.strip())
#   return salaries

  def extract_summary_from_result(soup): 
    summaries = []
    spans = soup.findAll("div", attrs={"class": "summary"})
    for span in spans:
      summaries.append(span.text.strip())
    return summaries

  def extract_job_age_from_result(soup):
    jobAge = []
    index = 0
    for div in soup.find_all(name="div", attrs={"class": "row"}):
      age = div.find_all(name="span", attrs={"class": "date"})
      for b in age:
        jobAge.append(b.text.strip())
    return jobAge





class CareerbuilderScraper():

  def extract_job_title_from_result(soup):
    jobs = []
    for div in soup.find_all(name="div", attrs={"id": "jobs_collection"}):
      for a in div.find_all(name="div", attrs={"class": "b"}):
        jobs.append(a.text)
    return jobs

  def extract_company_from_result(soup): 
    companies = []
    index = 0
    for div in soup.find_all(name="div", attrs={"id": "jobs_collection"}):
      company = div.find_all(name="span")
      for b in company:
        while index < 75:
          try:
            if company[index].text.strip().find('-') < 0:
              companies.append(company[index].text.strip())
            else:
              if company[index-1].text.strip() != 'Contractor':
                companies.append(company[index-1].text.strip())
              else:
                companies.append(None)
          except:
            return companies
          index +=3
    return companies

  def extract_refUrl_from_result(soup):
    urls = []
    for div in soup.find_all(name="div", attrs={"id": "jobs_collection"}):
      for a in div.find_all(name="a", attrs={"class": "block"}):
        urls.append(a['href'])
    return urls

  def extract_locations_from_result(soup): 
    locations = []
    index = 1
    for div in soup.find_all(name="div", attrs={"id": "jobs_collection"}):
      location = div.find_all(name="span")
      for b in location:
        while index < 75:
          try:
            if location[index].text.strip().find('-') < 0:
              locations.append(location[index-3].text.strip())
            elif location[index].text.strip() == 'Full-Time':
              locations.append(location[index-1].text.strip())
            else:
              locations.append(location[index].text.strip())
          except:
            return locations
          index +=3
    return locations

  def extract_jobPolicy_from_result(soup): 
    jobPolicy = []
    index = 2
    for div in soup.find_all(name="div", attrs={"id": "jobs_collection"}):
      policy = div.find_all(name="span")
      for b in policy:
        while index < 74:
          try:
            if len(policy[index].text.strip().split(' ')) > 1:
              if len(policy[index-1].text.strip().split(' ')) > 1:
                jobPolicy.append(policy[index-3].text.strip())
              else:
                jobPolicy.append(policy[index-1].text.strip())
            elif policy[index].text.strip().find('-') < 0 and len(policy[index].text.strip().split(' ')) < 2:
              jobPolicy.append(policy[index-3].text.strip())
            else:
              jobPolicy.append(policy[index].text.strip())
          except:
            return jobPolicy
          index +=3
    return jobPolicy

  def extract_salary_from_result(soup): 
    salaries = []
    index = 1
    for div in soup.find_all(name="div", attrs={"id": "jobs_collection"}):
      salary = div.find_all(name="div", attrs={"class": "block"})
      for b in salary:
        while index < 50:
          try:
            if salary[index].text.strip() == '':
              salaries.append('N/A')
            else:
              salaries.append(salary[index].text.strip())
          except:
            return salaries
          index+=2
    return salaries

  def extract_summary_from_result(soup): 
    summaries = []
    index = 0
    for div in soup.find_all(name="div", attrs={"id": "jobs_collection"}):
      summary = div.find_all(name="div", attrs={"class": "block"})
      for b in summary:
        while index < 50:
          try:
            summaries.append(summary[index].text.strip())
          except:
            return summaries
          index+=2
    return summaries

  def extract_job_age_from_result(soup):
    jobAge = []
    index = 0
    for div in soup.find_all(name="div", attrs={"id": "jobs_collection"}):
      age = div.find_all(name="div", attrs={"class": "data-results-publish-time"})
      for b in age:
        jobAge.append(b.text.strip())
    return jobAge

def isRemote(value):
  if value == 'true':
    return '&forceLocation=032b3046-06a3-4876-8dfd-474eb5e7ed11'
  else:
    return ''


if __name__ == '__main__':
  json = []
  responseJson = []
  iWillScrape = True
  cbWillScrape = True
  title = input("Job Title: ")
  salary = input("Salary Estimate: ")
  location = input("Location: ")
  remote = input("Remote?: ")
  jobType = input("Job Type: ")
  experience = input("Experience level: ")
  age = input("Date Posted: ")
  pageNumber = int(input("Start page: "))

  URL = "https://www.indeed.com/jobs?q="+title.replace(" ", "+")+"+"+"$"+salary+"&l="+location.replace(" ", "+")+"&jt="+jobType.replace("-", '').lower()+"&explvl="+experience.replace("-", "_").lower()+"&fromage="+age+isRemote(remote)+"&start="+str(pageNumber-10)
  print(URL)
  page = requests.get(URL)
  soup = BeautifulSoup(page.text, "html.parser")

  _URL = "https://www.careerbuilder.com/jobs?keywords="+title.replace(" ", "+")+"&pay="+salary.split(",")[0]+"&location="+location.replace(" ", "+")+"&emp=jt"+"ft"+"&posted="+age+"&cb_workhome="+str(remote).lower()+"&page_number="+str(int(pageNumber/10))
  print(_URL)
  _page = requests.get(_URL)
  _soup = BeautifulSoup(_page.text, "html.parser")

  iJobTitles = IndeedScraper.extract_job_title_from_result(soup)
  iCompanies = IndeedScraper.extract_company_from_result(soup)
  iLinks = IndeedScraper.extract_refUrl_from_result(soup)
  iLocations = IndeedScraper.extract_locations_from_result(soup)
  iJobPolicies = IndeedScraper.extract_jobPolicy_from_result(soup)
  iSummaries = IndeedScraper.extract_summary_from_result(soup)
  iJobAges = IndeedScraper.extract_job_age_from_result(soup)

  cbJobTitles = CareerbuilderScraper.extract_job_title_from_result(_soup)
  cbCompanies = CareerbuilderScraper.extract_company_from_result(_soup)
  cbLinks = CareerbuilderScraper.extract_refUrl_from_result(_soup)
  cbLocations = CareerbuilderScraper.extract_locations_from_result(_soup)
  cbJobPolicies = CareerbuilderScraper.extract_jobPolicy_from_result(_soup)
  cbSalaries = CareerbuilderScraper.extract_salary_from_result(_soup)
  cbSummaries = CareerbuilderScraper.extract_summary_from_result(_soup)
  cbJobAges = CareerbuilderScraper.extract_job_age_from_result(_soup)

  cnt =0
  while cnt < len(iJobTitles):
    json.append({"title": iJobTitles[cnt], "company": iCompanies[cnt], "source": "https://indeed.com", "refUrl": iLinks[cnt], "location": iLocations[cnt], "jobPolicy": iJobPolicies[cnt], "description": iSummaries[cnt], "datePosted": iJobAges[cnt], "salary": 'N/A'})
    cnt+=1

  cnt =0
  while cnt < len(cbJobTitles):
    try:
      json.append({"title": cbJobTitles[cnt], "company": cbCompanies[cnt], "source": "https://careerbuilder.com", "refUrl": cbLinks[cnt], "location": cbLocations[cnt], "jobPolicy": cbJobPolicies[cnt], "description": cbSummaries[cnt], "datePosted": cbJobAges[cnt], "salary": cbSalaries[cnt]})
    except:
      pass
    cnt+=1

  if len(iJobTitles) == 15:
    iWillScrape = True
  else:
    iWillScrape = False

  if len(cbJobTitles) == 25:
    cbWillScrape = True
  else:
    cbWillScrape = False


  responseJson = {
    "data": json,
    "iWillScrape": iWillScrape,
    "cbWillScrape": cbWillScrape,
  }

  print(responseJson)

